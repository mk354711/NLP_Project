import pandas as pd
from baseline import baseline_llm
from agent import agent_generate
from evaluation import evaluate
from tqdm import tqdm
from langchain_chroma import Chroma
from langchain_core.documents import Document
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper

file_name = "data/Aircraft_Annotation_DataFile.csv"
df = pd.read_csv(file_name)
df = df[['PROBLEM', 'ACTION']].dropna().reset_index(drop=True)

print(f"Successfully loaded {file_name} with {len(df)} rows. First 5 rows:")
print(df.head())

docs = [
    Document(
        page_content=row["PROBLEM"],
        metadata={"ACTION": row["ACTION"]}
    )
    for _, row in df.iterrows()
]

vectorstore = Chroma.from_documents(docs, embedding=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 12})

print("Vector DB êµ¬ì¶• ì™„ë£Œ!")


# ìƒ˜í”Œ í‰ê°€ìš© ë°ì´í„°
eval_df = df.sample(10, random_state=42)

questions = eval_df["PROBLEM"].tolist()
ground_truth = eval_df["ACTION"].tolist()

baseline_answers = []
agent_answers = []
contexts = []

for q in questions:
    # ğŸ”¹ Baseline
    base_ans = baseline_llm(q)
    baseline_answers.append(base_ans)

    # ğŸ”¹ Agent
    agent_ans = agent_app.invoke({"PROBLEM": q})["final_action"]
    agent_answers.append(agent_ans)

    # ğŸ”¹ Context (RAGìš©)
    docs = retriever.invoke(q)
    ctx = [d.page_content for d in docs]
    contexts.append(ctx)

baseline_dataset = Dataset.from_dict({
    "question": questions,
    "answer": baseline_answers,
    "contexts": contexts,
    "ground_truth": ground_truth
})

agent_dataset = Dataset.from_dict({
    "question": questions,
    "answer": agent_answers,
    "contexts": contexts,
    "ground_truth": ground_truth
})

print("Baseline / Agent í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")


ragas_llm = LangchainLLMWrapper(llm)
ragas_embed = LangchainEmbeddingsWrapper(embeddings)

print("â–¶ Baseline í‰ê°€ ì¤‘...")
baseline_results = evaluate(
    dataset=baseline_dataset,
    metrics=[faithfulness, answer_relevancy],
    llm=ragas_llm,
    embeddings=ragas_embed
)

print("â–¶ Agent í‰ê°€ ì¤‘...")
agent_results = evaluate(
    dataset=agent_dataset,
    metrics=[faithfulness, answer_relevancy],
    llm=ragas_llm,
    embeddings=ragas_embed
)

baseline_df = baseline_results.to_pandas()
agent_df = agent_results.to_pandas()

result_df = pd.DataFrame({
    "Fault": eval_df["PROBLEM"],
    "Ground Truth": eval_df["ACTION"],
    "Baseline Answer": baseline_answers,
    "Agent Answer": agent_answers
})

result_df.to_csv('data/result_df.csv')

print(baseline_df['answer_relevancy'].mean(), agent_df['answer_relevancy'].mean())