from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper

# ìƒ˜í”Œ í‰ê°€ìš© ë°ì´í„°
eval_df = df.sample(10, random_state=42)

questions = eval_df["PROBLEM"].tolist()
ground_truth = eval_df["ACTION"].tolist()

baseline_answers = []
agent_answers = []
contexts = []

for q in questions:
    # ğŸ”¹ Baseline
    base_ans = baseline_llm(q)
    baseline_answers.append(base_ans)

    # ğŸ”¹ Agent
    agent_ans = agent_app.invoke({"PROBLEM": q})["final_action"]
    agent_answers.append(agent_ans)

    # ğŸ”¹ Context (RAGìš©)
    docs = retriever.invoke(q)
    ctx = [d.page_content for d in docs]
    contexts.append(ctx)

baseline_dataset = Dataset.from_dict({
    "question": questions,
    "answer": baseline_answers,
    "contexts": contexts,
    "ground_truth": ground_truth
})

agent_dataset = Dataset.from_dict({
    "question": questions,
    "answer": agent_answers,
    "contexts": contexts,
    "ground_truth": ground_truth
})

print("Baseline / Agent í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")

ragas_llm = LangchainLLMWrapper(llm)
ragas_embed = LangchainEmbeddingsWrapper(embeddings)

print("â–¶ Baseline í‰ê°€ ì¤‘...")
baseline_results = evaluate(
    dataset=baseline_dataset,
    metrics=[faithfulness, answer_relevancy],
    llm=ragas_llm,
    embeddings=ragas_embed
)

print("â–¶ Agent í‰ê°€ ì¤‘...")
agent_results = evaluate(
    dataset=agent_dataset,
    metrics=[faithfulness, answer_relevancy],
    llm=ragas_llm,
    embeddings=ragas_embed
)

baseline_df = baseline_results.to_pandas()
agent_df = agent_results.to_pandas()

baseline_df, agent_df
